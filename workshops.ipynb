{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read OpenAI API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"secrets.env\")\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call to OpenAI API\n",
    "\n",
    "**Advantages and disadvantages of using OpenAI API:**\n",
    "\n",
    "**Pros**\n",
    "\n",
    "1. **Easy to use:** The API is well-documented and easy to integrate into various applications.\n",
    "2. **Versatility**: The API can be used for a wide range of tasks, including text generation, translation, image generation, code explanation, and more.\n",
    "3. **Continuous Improvement:** OpenAI continually updates and improves its models, so your chatbot can benefit from cutting-edge advancements in AI without needing significant internal resources.\n",
    "\n",
    "**Cons**\n",
    "\n",
    "1. **Cost:** Using the API extensively can get expensive. You'll be charged based on the number of tokens or images processed.\n",
    "2. **No customization options:** You can't control or fine-tune how models are trained. You're limited to the models provided by OpenAI.\n",
    "3. **Data Privacy Concerns:** Using a third-party API may raise concerns about data privacy and security, particularly in industries that handle sensitive customer information.\n",
    "4. **Dependence on a third party service:** Relying on an external API means you're dependent on its uptime, updates, and potential changes in pricing or terms of service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is AI?\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"gpt-3.5-turbo\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about OpenAI API check this [link](https://platform.openai.com/docs/overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do I need to write utility classes on my own? - Utilise LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", api_key=OPENAI_API_KEY)\n",
    "response = model.invoke([HumanMessage(content=\"What is AI?\")])\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke([HumanMessage(content=\"It is really interesting what you have written. Can you say more about it?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see - the model by itself doesn't remember the history of messages. We need to pass all messages to get the desired answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "response = model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"What is AI?\"),\n",
    "        AIMessage(\n",
    "            content=\"AI, or artificial intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. AI technology aims to mimic human cognitive functions and improve efficiency and accuracy in tasks that were previously only achievable by humans.\"\n",
    "        ),\n",
    "        HumanMessage(content=\"It is really interesting what you have written. Can you say more about it?\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More on integrating with OpenAI using Langchain [here](https://python.langchain.com/v0.2/docs/integrations/chat/openai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import (\n",
    "    BaseChatMessageHistory,\n",
    "    InMemoryChatMessageHistory,\n",
    ")\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(model, get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"session_1\"}}\n",
    "\n",
    "response = chain_with_history.invoke(\n",
    "    [HumanMessage(content=\"What is AI?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_with_history.invoke(\n",
    "    [HumanMessage(content=\"It is really interesting what you have written. Can you say more about it?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a session_id change, another user will not have access to the previous conversation history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"session_2\"}}\n",
    "\n",
    "response = chain_with_history.invoke(\n",
    "    [HumanMessage(content=\"What was I asking about earlier?\")],\n",
    "    config=config,\n",
    ")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All on message history with Langchain [here](https://python.langchain.com/v0.2/docs/how_to/message_history/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different types of memories\n",
    "\n",
    "- all messages (presented earlier)\n",
    "- trim messages\n",
    "    - (+) helps to reduce context\n",
    "    - (-) older parts of conversation will be discarded and not passed to LLM\n",
    "- summarize conversation\n",
    "    - (+) helps to reduce context\n",
    "    - (-) some parts of conversation during summarization might be lost, but will better preserve older parts of conversation than trimming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trim messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "# Trim to four messages\n",
    "def trim_messages(chain_input):\n",
    "    current_store = store[config[\"configurable\"][\"session_id\"]]\n",
    "    if len(current_store.messages) <= 4:\n",
    "        return False\n",
    "\n",
    "    current_store.clear()\n",
    "\n",
    "    for message in current_store.messages[-4:]:\n",
    "        current_store.add_message(message)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "chain_with_trimming = RunnablePassthrough.assign(messages_trimmed=trim_messages) | chain_with_history\n",
    "\n",
    "response = chain_with_trimming.invoke(\n",
    "    {\"input\": \"My name is Bob.\"},\n",
    "    config=config,\n",
    ")\n",
    "print(f\"1: {response.content}\")\n",
    "\n",
    "response = chain_with_trimming.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(f\"2: {response.content}\")\n",
    "\n",
    "response = chain_with_trimming.invoke(\n",
    "    {\"input\": \"What is my name?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(f\"3: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "def summarize_messages(chain_input):\n",
    "    current_store = store[config[\"configurable\"][\"session_id\"]]\n",
    "    if len(current_store.messages) == 0:\n",
    "        return False\n",
    "\n",
    "    summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\n",
    "                \"user\",\n",
    "                \"Distill the above chat messages into a single summary message. Include as many specific details as you can.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    summarization_chain = summarization_prompt | model\n",
    "    summary_message = summarization_chain.invoke({\"chat_history\": current_store.messages})\n",
    "\n",
    "    current_store.clear()\n",
    "    current_store.add_message(summary_message)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "chain_with_summarization = RunnablePassthrough.assign(messages_summarized=summarize_messages) | chain_with_history\n",
    "\n",
    "response = chain_with_summarization.invoke(\n",
    "    {\"input\": \"My name is Bob.\"},\n",
    "    config=config,\n",
    ")\n",
    "print(f\"1: {response.content}\")\n",
    "\n",
    "response = chain_with_summarization.invoke(\n",
    "    {\"input\": \"With what name did I introduce myself?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(f\"2: {response.content}\")\n",
    "\n",
    "response = chain_with_summarization.invoke(\n",
    "    {\"input\": \"With what name did I introduce myself?\"},\n",
    "    config=config,\n",
    ")\n",
    "print(f\"3: {response.content}\")\n",
    "print(store[config[\"configurable\"][\"session_id\"]].messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting techniques\n",
    "\n",
    "- Use delimiters to clearly indicate distinct parts of the input\n",
    "- Ask for a structured output\n",
    "- Ask the model to check whether conditions are satisfied\n",
    "- \"Few-shot\" prompting\n",
    "- Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use delimiters to clearly indicate distinct parts of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "You should express what you want a model to do by\n",
    "providing instructions that are as clear and\n",
    "specific as you can possibly make them.\n",
    "This will guide the model towards the desired output,\n",
    "and reduce the chances of receiving irrelevant\n",
    "or incorrect responses. Don't confuse writing a\n",
    "clear prompt with writing a short prompt.\n",
    "In many cases, longer prompts provide more clarity\n",
    "and context for the model, which can lead to\n",
    "more detailed and relevant outputs.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Summarize the text delimited by triple backticks \\\n",
    "into a single sentence.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask for a structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Generate a list of three made-up book titles along with their number of pages (int), release date (yyyy-mm-dd) and if cover is hard or not (boolean).\n",
    "Provide them in a yaml structured output format where title name is a key with three elemnts: number of pages, release date and if cover type.\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - try to rewrite this prompt to get the results in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Place for your prompt\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask the model to check whether conditions are satisfied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = f\"\"\"\n",
    "Preheat the oven to 350°F (175°C).\n",
    "Mix 1 cup of softened butter, 1 cup of sugar, 2 cups of flour, and 1 tsp vanilla extract until combined.\n",
    "Scoop spoonfuls onto a baking sheet and bake for 10-12 minutes, until golden brown.\n",
    "Let cool, and enjoy!\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes.\n",
    "If it contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions,\n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_1}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = f\"\"\"\n",
    "A short story is a piece of prose fiction. It can typically be read\n",
    "in a single sitting and focuses on a self-containedincident or series\n",
    "of linked incidents, with the intent of evoking a single effect or mood.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will be provided with text delimited by triple quotes.\n",
    "If it contains a sequence of instructions,\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Step 1 - ...\n",
    "Step 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions,\n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Few-shot\" prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<child>: Teach me about patience.\n",
    "\n",
    "<grandparent>: The river that carves the deepest\n",
    "valley flows from a modest spring; the\n",
    "grandest symphony originates from a single note;\n",
    "the most intricate tapestry begins with a solitary thread.\n",
    "\n",
    "<child>: Teach me about perseverance.\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the steps required to complete a task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Preheat the oven to 350°F (175°C).\n",
    "Mix 1 cup of softened butter, 1 cup of sugar, 2 cups of flour, and 1 tsp vanilla extract until combined.\n",
    "Scoop spoonfuls onto a baking sheet and bake for 10-12 minutes, until golden brown.\n",
    "Let cool, and enjoy!\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "I will give you text delimited with ###.\n",
    "Perform the following actions:\n",
    "\n",
    "1 - If it contains the instruction, rewrite them in the following format:\n",
    "\n",
    "A - <instruction 1>\n",
    "B - <instruction 2>\n",
    "C - <instruction 3>\n",
    "\n",
    "2 - Translate each instruction to one of the following languages: spanish, french, german, polish, norwegian.\n",
    "3 - Output a list of objects in a json format. Each object in a list is for one translated sentence and contains the following keys:\n",
    "original_sentence, translated_sentence, translation_language. There should be as many objects as instructions in the previous step.\n",
    "4 - If it does not contin instructions output json object with a key \\\"error\" and value for it which will be an error message\n",
    "\n",
    "###{text}###\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - Write a prompt that generates three different made-up movies and their descriptions. Each movie name and description should be in a different style (poem, pirate-like, William Shakespeare). The output should be an HTML table. Use all of the aforementioned techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Place for your prompt.\n",
    "\"\"\"\n",
    "\n",
    "response = model.invoke([HumanMessage(content=prompt)])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about prompting techniques check [this website](https://www.promptingguide.ai/techniques)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful assistant and a pirate. Answer all questions to the best of your ability, but remember to use pirate-like english.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "pirate_chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "response = pirate_chain_with_history.invoke({\"input\": \"What is AI?\"}, config=config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Langchain prompt templates [guide](https://python.langchain.com/v0.2/docs/concepts/#prompt-templates)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build RAG - add vector storage\n",
    "## Initialize vector storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"shrek_collection\", embedding_function=embeddings, persist_directory=\"./chroma_langchain_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating storage where text chunks are quite big...\n",
    "Creating larger chunks results in more text being passed as context. It may improve the accuracy of our LLM's answer, but it increases the cost of the API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = PyPDFLoader(\"./docs/shrek-script.pdf\")\n",
    "document = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "chunked_documents = text_splitter.split_documents(document)\n",
    "\n",
    "vector_store.add_documents(chunked_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask and wait for answears :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"session_id\": \"rag_session_1\"}}\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a helpful assistant and a literature specialist. Answer all questions to the best of your ability.\n",
    "            During answearing use this context from documents: {context}. If you do not know the answear - do not provide it.\n",
    "            Answear as short as possible, preferably in one sentence\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "rag_chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | rag_chain_with_history\n",
    ")\n",
    "\n",
    "response = rag_chain.invoke(\"What was the name of the princess?\", config=config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to make the chunk size smaller to reduce costs by providing fewer tokens in context. As you will see, larger chunks do not always give better results.\n",
    "You have to find a chunk size that is not too big (we don't want our context to be too big) and that is sufficient for our LLM to get an answer from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"shrek_collection_small\", embedding_function=embeddings, persist_directory=\"./chroma_langchain_db\"\n",
    ")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "chunked_documents = text_splitter.split_documents(document)\n",
    "\n",
    "vector_store.add_documents(chunked_documents)\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"rag_session_2\"}}\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "rag_chain = {\n",
    "    \"context\": retriever | format_docs,\n",
    "    \"input\": RunnablePassthrough(),\n",
    "} | rag_chain_with_history\n",
    "\n",
    "response = rag_chain.invoke(\"What was the name of the princess shrek wanted to resque?\", config=config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more about using Chroma with Langchain in [docs](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector storage search types\n",
    "- similarity (default)\n",
    "- mmr\n",
    "- similarity_score_threshold\n",
    "\n",
    "More on this topic [here](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task - set up how many documents (3) should be retrieved from our vector storage using \"mmr\" search type\n",
    "\n",
    "Try to set up how many documents should be retrieved from our vector storage (and passed to our prompt). It is another hyperparameter which when well tuned can help us reduce api call costs without losing answear precision. Also - change used search algorithm to \"mmr\".\n",
    "[Link to docs](https://python.langchain.com/v0.2/docs/integrations/vectorstores/chroma/#query-directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place for your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to learn more about components used in this section, check out those links:\n",
    "\n",
    "- [document loaders](https://python.langchain.com/v0.2/docs/how_to/#document-loaders)\n",
    "- [text splitters](https://python.langchain.com/v0.2/docs/how_to/#text-splitters)\n",
    "- [embedding models](https://python.langchain.com/v0.2/docs/how_to/#embedding-models)\n",
    "- [retrievers](https://python.langchain.com/v0.2/docs/how_to/#embedding-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Put it all together - Demo with Frontend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panel as pn\n",
    "\n",
    "pn.extension()\n",
    "\n",
    "\n",
    "async def callback(contents: str, user: str, instance: pn.chat.ChatInterface):\n",
    "    message = \"\"\n",
    "    for response in rag_chain.stream(contents, config=config):\n",
    "        message += response.content\n",
    "        yield message\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"rag_session_2\"}}\n",
    "chat_interface = pn.chat.ChatInterface(callback=callback, callback_user=\"Shrek director\")\n",
    "chat_interface.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional task - use HuggingFace model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "HF_TOKEN = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        max_new_tokens=512,\n",
    "        return_full_text=False,\n",
    "        do_sample=True,\n",
    "        repetition_penalty=1.03,\n",
    "        temperature=0.1\n",
    "    ),\n",
    ")\n",
    "\n",
    "hg_model = ChatHuggingFace(llm=llm)\n",
    "chain = prompt | hg_model\n",
    "hg_chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "rag_chain = {\n",
    "    \"context\": retriever | format_docs,\n",
    "    \"input\": RunnablePassthrough(),\n",
    "} | hg_chain_with_history\n",
    "\n",
    "response = rag_chain.invoke(\"Who was Shrek's partner when rescuing Fiona?\", config=config)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested, check more information about integrating with Hugging Face:\n",
    "\n",
    "- [Integrating with Hugging Face using Langchain](https://python.langchain.com/v0.1/docs/integrations/platforms/huggingface/)\n",
    "- [Hugging Face docs](https://huggingface.co/docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
